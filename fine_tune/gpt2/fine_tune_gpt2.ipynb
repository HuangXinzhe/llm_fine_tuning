{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入相关库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import TrainingArguments, Seq2SeqTrainingArguments\n",
    "from transformers import Trainer, Seq2SeqTrainer\n",
    "import transformers\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import TextGenerationPipeline\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 7.46k/7.46k [00:00<00:00, 6.56MB/s]\n",
      "Downloading data: 100%|██████████| 699k/699k [00:10<00:00, 67.1kB/s]\n",
      "Downloading data: 100%|██████████| 90.0k/90.0k [00:03<00:00, 28.9kB/s]\n",
      "Downloading data: 100%|██████████| 92.2k/92.2k [00:03<00:00, 29.5kB/s]\n",
      "Generating train split: 100%|██████████| 8530/8530 [00:00<00:00, 382290.42 examples/s]\n",
      "Generating validation split: 100%|██████████| 1066/1066 [00:00<00:00, 398035.08 examples/s]\n",
      "Generating test split: 100%|██████████| 1066/1066 [00:00<00:00, 488061.14 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# 数据集名称\n",
    "DATASET_NAME = \"rotten_tomatoes\"\n",
    "\n",
    "# 加载数据集\n",
    "raw_datasets = load_dataset(DATASET_NAME, cache_dir=\"/Volumes/WD_BLACK/data/rotten_tomatoes\")\n",
    "\n",
    "# 训练集\n",
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "\n",
    "# 验证集\n",
    "raw_valid_dataset = raw_datasets[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "print(raw_train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 8530\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text', 'label']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset.column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型名称\n",
    "# MODEL_NAME = \"gpt2\"\n",
    "MODEL_NAME_OR_PATH = \"/Volumes/WD_BLACK/models/gpt2\"\n",
    "\n",
    "# 加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME_OR_PATH, trust_remote_code=True)  # trust_remote_code=True表示信任远程代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 加载Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_OR_PATH, trust_remote_code=True)\n",
    "\n",
    "# 在GPT2中没有pad_token，需要手动添加\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # 添加特殊token\n",
    "tokenizer.pad_token_id = 0  # 设置pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 其它相关公共变量赋值\n",
    "\n",
    "# 设置随机种子：同个种子的随机序列可复现\n",
    "transformers.set_seed(42)\n",
    "\n",
    "# 标签集\n",
    "named_labels = ['neg', 'pos']\n",
    "\n",
    "# 标签转 token_id\n",
    "label_ids = [\n",
    "    tokenizer(named_labels[i], add_special_tokens=False)[\"input_ids\"][0]\n",
    "    for i in range(len(named_labels))\n",
    "]  # add_special_tokens=False表示不添加特殊token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 处理数据集\n",
    "转成模型接受的输入格式\n",
    "   - 拼接输入输出：\\<INPUT TOKEN IDS\\>\\<EOS_TOKEN_ID\\>\\<OUTPUT TOKEN IDS\\>\n",
    "   - PAD 成相等长度：\n",
    "     - <INPUT 1.1><INPUT 1.2>...\\<EOS_TOKEN_ID\\>\\<OUTPUT TOKEN IDS\\>\\<PAD\\>...\\<PAD\\>\n",
    "     - <INPUT 2.1><INPUT 2.2>...\\<EOS_TOKEN_ID\\>\\<OUTPUT TOKEN IDS\\>\\<PAD\\>...\\<PAD\\>\n",
    "   - 标识出参与 Loss 计算的 Tokens (只有输出 Token 参与 Loss 计算)\n",
    "     - \\<-100\\>\\<-100\\>...\\<OUTPUT TOKEN IDS\\>\\<-100\\>...\\<-100\\>\n",
    "     - 除了输出其他都标记为-100，是Huggingface预留的标记\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN=32   #最大序列长度（输入+输出）\n",
    "DATA_BODY_KEY = \"text\" # 数据集中的输入字段名\n",
    "DATA_LABEL_KEY = \"label\" #数据集中输出字段名\n",
    "\n",
    "# 定义数据处理函数，把原始数据转成input_ids, attention_mask, labels\n",
    "def process_fn(examples):\n",
    "    model_inputs = {\n",
    "            \"input_ids\": [],\n",
    "            \"attention_mask\": [],\n",
    "            \"labels\": [],\n",
    "        }\n",
    "    for i in range(len(examples[DATA_BODY_KEY])):\n",
    "        inputs = tokenizer(examples[DATA_BODY_KEY][i],add_special_tokens=False)\n",
    "        label = label_ids[examples[DATA_LABEL_KEY][i]]\n",
    "        input_ids = inputs[\"input_ids\"] + [tokenizer.eos_token_id, label]\n",
    "\n",
    "        raw_len = len(input_ids)\n",
    "        input_len = len(inputs[\"input_ids\"]) + 1 # 加1是因为eos_token_id\n",
    "\n",
    "        if raw_len >= MAX_LEN:\n",
    "            input_ids = input_ids[-MAX_LEN:]  # 当长度超过最大长度时，只取后面的最大长度\n",
    "            attention_mask = [1] * MAX_LEN\n",
    "            labels = [-100]*(MAX_LEN - 1) + [label]\n",
    "        else:\n",
    "            input_ids = input_ids + [tokenizer.pad_token_id] * (MAX_LEN - raw_len)\n",
    "            attention_mask = [1] * raw_len + [0] * (MAX_LEN - raw_len)\n",
    "            labels = [-100]*input_len + [label] + [-100] * (MAX_LEN - raw_len)\n",
    "        model_inputs[\"input_ids\"].append(input_ids)\n",
    "        model_inputs[\"attention_mask\"].append(attention_mask)\n",
    "        model_inputs[\"labels\"].append(labels)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on train dataset: 100%|██████████| 8530/8530 [00:00<00:00, 11860.61 examples/s]\n",
      "Running tokenizer on validation dataset: 100%|██████████| 1066/1066 [00:00<00:00, 11383.43 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# 处理训练数据集\n",
    "tokenized_train_dataset = raw_train_dataset.map(\n",
    "    process_fn,\n",
    "    batched=True,\n",
    "    remove_columns=raw_train_dataset.column_names,  # 已经对数据进行了处理，去除原有数据中的列，只保留处理后的列\n",
    "    desc=\"Running tokenizer on train dataset\",\n",
    ")\n",
    "\n",
    "# 处理验证数据集\n",
    "tokenized_valid_dataset = raw_valid_dataset.map(\n",
    "    process_fn,\n",
    "    batched=True,\n",
    "    remove_columns=raw_valid_dataset.column_names,\n",
    "    desc=\"Running tokenizer on validation dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 定义数据规整器\n",
    "训练时自动将数据拆分成 Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据校准器（自动生成batch）\n",
    "collater = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer, \n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 定义训练超参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 2e-5         # 学习率\n",
    "BATCH_SIZE = 8    # Batch大小\n",
    "INTERVAL = 100    # 每多少步打一次 log / 做一次 eval\n",
    "\n",
    "# 定义训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",              # checkpoint保存路径\n",
    "    evaluation_strategy=\"steps\",        # 按步数计算eval频率\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,                 # 训练epoch数\n",
    "    per_device_train_batch_size=BATCH_SIZE,     # 每张卡的batch大小\n",
    "    gradient_accumulation_steps=1,              # 累加几个step做一次参数更新\n",
    "    per_device_eval_batch_size=BATCH_SIZE,      # evaluation batch size\n",
    "    eval_steps=INTERVAL,                # 每N步eval一次\n",
    "    logging_steps=INTERVAL,             # 每N步log一次\n",
    "    save_steps=INTERVAL,                # 每N步保存一个checkpoint\n",
    "    learning_rate=LR,                   # 学习率\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 定义训练器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# 节省显存\n",
    "model.gradient_checkpointing_enable()  # 开启梯度检查点，当反向传播时，重新计算中间激活值\n",
    "\n",
    "# 定义训练器\n",
    "trainer = Trainer(\n",
    "    model=model,  # 待训练模型\n",
    "    args=training_args,  # 训练参数\n",
    "    data_collator=collater,  # 数据校准器\n",
    "    train_dataset=tokenized_train_dataset,  # 训练集\n",
    "    eval_dataset=tokenized_valid_dataset,   # 验证集\n",
    "    # compute_metrics=compute_metric,         # 计算自定义评估指标\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m315680524\u001b[0m (\u001b[33m550w\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/huangxinzhe/code/llm_fine_tuning/fine_tune/gpt2/wandb/run-20240327_224936-ghfnk16z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/550w/huggingface/runs/ghfnk16z' target=\"_blank\">prime-moon-4</a></strong> to <a href='https://wandb.ai/550w/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/550w/huggingface' target=\"_blank\">https://wandb.ai/550w/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/550w/huggingface/runs/ghfnk16z' target=\"_blank\">https://wandb.ai/550w/huggingface/runs/ghfnk16z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1067 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/Users/huangxinzhe/opt/anaconda3/envs/llm10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "wandb: Network error (TransientError), entering retry loop.\n",
      "  9%|▉         | 100/1067 [00:35<04:35,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0572, 'learning_rate': 1.8125585754451735e-05, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      "  9%|▉         | 100/1067 [00:42<04:35,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.021487252786755562, 'eval_runtime': 7.3399, 'eval_samples_per_second': 145.234, 'eval_steps_per_second': 18.256, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huangxinzhe/opt/anaconda3/envs/llm10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 19%|█▊        | 200/1067 [01:14<04:28,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0234, 'learning_rate': 1.6251171508903468e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 19%|█▊        | 200/1067 [01:21<04:28,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.017513608559966087, 'eval_runtime': 6.4411, 'eval_samples_per_second': 165.5, 'eval_steps_per_second': 20.804, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huangxinzhe/opt/anaconda3/envs/llm10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 28%|██▊       | 300/1067 [01:53<03:41,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0172, 'learning_rate': 1.4376757263355203e-05, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 28%|██▊       | 300/1067 [02:00<03:41,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.012730359099805355, 'eval_runtime': 6.654, 'eval_samples_per_second': 160.204, 'eval_steps_per_second': 20.138, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huangxinzhe/opt/anaconda3/envs/llm10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 37%|███▋      | 400/1067 [02:33<03:22,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0147, 'learning_rate': 1.2502343017806936e-05, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 37%|███▋      | 400/1067 [02:40<03:22,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.012395706959068775, 'eval_runtime': 6.9641, 'eval_samples_per_second': 153.07, 'eval_steps_per_second': 19.241, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huangxinzhe/opt/anaconda3/envs/llm10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 47%|████▋     | 500/1067 [03:12<02:48,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0127, 'learning_rate': 1.0627928772258671e-05, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 47%|████▋     | 500/1067 [03:19<02:48,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.013758053071796894, 'eval_runtime': 6.9672, 'eval_samples_per_second': 153.002, 'eval_steps_per_second': 19.233, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huangxinzhe/opt/anaconda3/envs/llm10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 56%|█████▌    | 600/1067 [03:52<02:14,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0151, 'learning_rate': 8.753514526710405e-06, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 56%|█████▌    | 600/1067 [03:58<02:14,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.011735978536307812, 'eval_runtime': 6.2788, 'eval_samples_per_second': 169.778, 'eval_steps_per_second': 21.342, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huangxinzhe/opt/anaconda3/envs/llm10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 66%|██████▌   | 700/1067 [04:28<01:45,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0133, 'learning_rate': 6.879100281162138e-06, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 66%|██████▌   | 700/1067 [04:35<01:45,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.012164085172116756, 'eval_runtime': 6.2543, 'eval_samples_per_second': 170.443, 'eval_steps_per_second': 21.425, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huangxinzhe/opt/anaconda3/envs/llm10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 75%|███████▍  | 800/1067 [05:05<01:14,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0133, 'learning_rate': 5.004686035613872e-06, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 75%|███████▍  | 800/1067 [05:11<01:14,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.011407976970076561, 'eval_runtime': 6.2414, 'eval_samples_per_second': 170.796, 'eval_steps_per_second': 21.47, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huangxinzhe/opt/anaconda3/envs/llm10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 84%|████████▍ | 900/1067 [05:42<00:47,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0129, 'learning_rate': 3.1302717900656047e-06, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 84%|████████▍ | 900/1067 [05:48<00:47,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.011636830866336823, 'eval_runtime': 6.2606, 'eval_samples_per_second': 170.27, 'eval_steps_per_second': 21.404, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huangxinzhe/opt/anaconda3/envs/llm10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 94%|█████████▎| 1000/1067 [06:20<00:21,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0121, 'learning_rate': 1.2558575445173386e-06, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 94%|█████████▎| 1000/1067 [06:28<00:21,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.011485500261187553, 'eval_runtime': 7.5901, 'eval_samples_per_second': 140.446, 'eval_steps_per_second': 17.655, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huangxinzhe/opt/anaconda3/envs/llm10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "100%|██████████| 1067/1067 [06:50<00:00,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 418.9792, 'train_samples_per_second': 20.359, 'train_steps_per_second': 2.547, 'train_loss': 0.018753886334563152, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1067, training_loss=0.018753886334563152, metrics={'train_runtime': 418.9792, 'train_samples_per_second': 20.359, 'train_steps_per_second': 2.547, 'train_loss': 0.018753886334563152, 'epoch': 1.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 开始训练\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 加载训练后的模型进行推理（参考）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 加载训练后的 checkpoint\n",
    "model = AutoModelForCausalLM.from_pretrained(\"output/checkpoint-1000\")\n",
    "\n",
    "# 模型设为推理模式\n",
    "model.eval()\n",
    "\n",
    "# 加载 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/Volumes/WD_BLACK/models/gpt2\")\n",
    "\n",
    "# 待分类文本\n",
    "text = \"This is a good movie!\"\n",
    "\n",
    "# 文本转 token ids - 记得以 eos 标识输入结束，与训练时一样\n",
    "inputs = tokenizer(text+tokenizer.eos_token, return_tensors=\"pt\")\n",
    "\n",
    "# 推理：预测标签\n",
    "output = model.generate(**inputs, do_sample=False, max_new_tokens=1)\n",
    "\n",
    "# label token 转标签文本\n",
    "tokenizer.decode(output[0][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 加载 checkpoint 并继续训练（选）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=\"/path/to/checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结上述过程\n",
    "\n",
    "1. 加载数据集\n",
    "2. 数据预处理：\n",
    "   - 将输入输出按特定格式拼接\n",
    "   - 文本转 Token IDs\n",
    "   - 通过 labels 标识出哪部分是输出（只有输出的 token 参与 loss 计算）\n",
    "3. 加载模型、Tokenizer\n",
    "4. 定义数据规整器\n",
    "5. 定义训练超参：学习率、批次大小、...\n",
    "6. 定义训练器\n",
    "7. 开始训练\n",
    "8. 注意：训练后推理时，输入数据的拼接方式要与训练时一致"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
