{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据筛选\n",
    "使用[Deita](https://github.com/hkust-nlp/deita)这个数据筛选工具，从已有的大规模SFT数据集中筛选出小规模、高质量的部分  \n",
    "\n",
    "Deita工具源于`《WHAT MAKES GOOD DATA FOR ALIGNMENT? A COMPREHENSIVE STUDY OF AUTOMATIC DATA SELECTION IN INSTRUCTION TUNING》`  \n",
    "\n",
    "## Deita工具的核心思想\n",
    "1. 按照指令复杂度和回复质量对所有的数据排序\n",
    "2. 按照顺序依次访问池内的数据，如果相似度小于阈值即可加入到最终的数据池中。\n",
    "\n",
    "## Deita安装\n",
    "pip install delta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Deita工具对以下三个指标进行筛选：\n",
    "1. 数据复杂度\n",
    "2. 数据质量\n",
    "3. 数据多样性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 数据复杂度\n",
    "数据复杂度主要是通过[hkust-nlp/deita-complexity-scorer](https://huggingface.co/hkust-nlp/deita-complexity-scorer)这个模型进行评估，该模型是基于Llama1-13B继续训练得到的。\n",
    "\n",
    "复杂度打分模型的训练方式为：\n",
    "\n",
    "1. 通过提示ChatGPT不断增加当前指令的复杂度，每个原始样本逐渐演进了5个更复杂的样本，共收集了6000个样本。使用的提示词主要负责添加限制或者提高问题的深度：\n",
    "```\n",
    "I want you act as a Prompt Rewriter.\n",
    "Your objective is to rewrite a given prompt into a more complex version to\n",
    "make those famous AI systems (e.g., ChatGPT and GPT4) a bit harder to handle.\n",
    "But the rewritten prompt must be reasonable and must be understood and\n",
    "responded by humans.\n",
    "Your rewriting cannot omit the non-text parts such as the table and code in\n",
    "#Given Prompt#:. Also, please do not omit the input in #Given Prompt#.\n",
    "You SHOULD complicate the given prompt using the following method:\n",
    "Please add one more constraints/requirements into #Given Prompt#\n",
    "You should try your best not to make the #Rewritten Prompt# become verbose,\n",
    "#Rewritten Prompt# can only add 10 to 20 words into #Given Prompt#.\n",
    "‘#Given Prompt#’, ‘#Rewritten Prompt#’, ‘given prompt’ and ‘rewritten prompt’\n",
    "are not allowed to appear in #Rewritten Prompt#\n",
    "#Given Prompt#:\n",
    "<Here is instruction>\n",
    "#Rewritten Prompt#:\n",
    "```\n",
    "2. 收集完样本后，组织成如下的形式，微调Llama1模型。\n",
    "```\n",
    "You are a helpful assistant. Please identify the complexity score of the following user query. \\n##Query: {instruction}  \\n##Complexity: \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "model_name = \"hkust-nlp/deita-complexity-scorer\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, load_in_4bit=True, device_map=\"auto\")  # 当显存不足时，使用load_in_4bit=True是将模型加载到4位精度，可以减少显存占用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们调用transformers自带的推理方法，基于Greedy Search生成各分数的概率，并加权求和作为模型对指令复杂度的最终打分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_complexity(model, tokenizer, input_text):\n",
    "    complexity_template = (\"You are a helpful assistant. Please identify the complexity score of the following user query. \\n##Query: {instruction}  \\n##Complexity: \")\n",
    "    user_input = complexity_template.format(instruction=input_text)\n",
    "    input_ids = tokenizer.encode(user_input, return_tensors=\"pt\")\n",
    "    print(\"用户输入:\", user_input)\n",
    "    print(\"用户输入分词后:\", input_ids)\n",
    "    \n",
    "    max_length = 512\n",
    "    outputs = model.generate(input_ids, \n",
    "                             max_length=max_length, \n",
    "                             num_return_sequences=1, \n",
    "                             return_dict_in_generate=True, \n",
    "                             output_scores=True)\n",
    "    logprobs_list = outputs.scores[0][0]\n",
    "    print(\"生成结果:\", logprobs_list)\n",
    "    score_logits = []\n",
    "    id2score = {\n",
    "        29896: \"1\",\n",
    "        29906: \"2\",\n",
    "        29941: \"3\",\n",
    "        29946: \"4\",\n",
    "        29945: \"5\",\n",
    "        29953: \"6\"\n",
    "    }\n",
    "    score_template = np.array([1,2,3,4,5,6])\n",
    "    for k in id2score:\n",
    "        score_logits.append(logprobs_list[k])\n",
    "    score_logits = np.array(score_logits)\n",
    "    score_npy = softmax(score_logits, axis=0)\n",
    "    print(\"取出1-6分对应位置模型预测的logits:\", score_logits)\n",
    "    print(\"取出1-6分对应位置模型预测的概率:\", score_npy)\n",
    "    score_npy = score_npy * score_template\n",
    "\n",
    "    score_npy = np.sum(score_npy, axis=0)\n",
    "    print(\"最终分数等于基于概率对1-6分的加权和:\", score_npy)\n",
    "    return score_npy\n",
    "\n",
    "input_text = \"write a performance review for a junior data scientist\"\n",
    "complexity_score = infer_complexity(model, tokenizer, input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出结果：\n",
    "```python\n",
    "用户输入: You are a helpful assistant. Please identify the complexity score of the following user query. \n",
    "##Query: write a performance review for a junior data scientist  \n",
    "##Complexity: \n",
    "用户输入分词后: tensor([[    1,   887,   526,   263,  8444, 20255, 29889,  3529, 12439,   278,\n",
    "         13644,  8158,   310,   278,  1494,  1404,  2346, 29889, 29871,    13,\n",
    "          2277,  3010, 29901,  2436,   263,  4180,  9076,   363,   263, 20183,\n",
    "           848,  9638,   391,   259,    13,  2277,  8909, 29916,   537, 29901,\n",
    "         29871]])\n",
    "生成结果: tensor([ -7.9375, -23.4375,   8.3828,  ...,  -4.6406,  -2.2578,  -2.6875])\n",
    "取出1-6分对应位置模型预测的logits: [18.859375  24.484375  21.453125  15.9296875 14.0078125 12.984375 ]\n",
    "取出1-6分对应位置模型预测的概率: [3.4279895e-03 9.5048648e-01 4.5865994e-02 1.8310170e-04 2.6793698e-05\n",
    " 9.6285166e-06]\n",
    "最终分数等于基于概率对1-6分的加权和: 2.042923080154651\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 数据质量\n",
    "数据质量方面，也采用和数据复杂度类似的筛选策略，通过ChatGPT不断生成更高质量的数据，例如使用如下的模板提升模型的Helpfulness：\n",
    "\n",
    "```\n",
    "I want you to act as a Response Rewriter\n",
    "Your goal is to enhance the quality of the response given by an AI assistant\n",
    "to the #Given Prompt# through rewriting.\n",
    "But the rewritten response must be reasonable and must be understood by humans.\n",
    "Your rewriting cannot omit the non-text parts such as the table and code in\n",
    "#Given Prompt# and #Given Response#. Also, please do not omit the input\n",
    "in #Given Prompt#.\n",
    "You Should enhance the quality of the response using the following method:\n",
    "Please make the Response more helpful to the user.\n",
    "You should try your best not to make the #Rewritten Response# become verbose,\n",
    "#Rewritten Response# can only add 10 to 20 words into #Given Response#.\n",
    "‘#Given Response#’, ‘#Rewritten Response#’, ‘given response’ and ‘rewritten response’\n",
    "are not allowed to appear in #Rewritten Response#\n",
    "#Given Prompt#:\n",
    "Give three tips for staying healthy.\n",
    "#Given Response#:\n",
    "<Response>\n",
    "#Rewritten Response#:\n",
    "```\n",
    "\n",
    "质量方面主要从以下几个角度提升，包括Helpfulness、Relevance、Depth。在得到质量不断提升的数据后，再利用他们构建带有分数的数据，训练一个质量打分器。官方提供的[hkust-nlp/deita-quality-scorer](https://huggingface.co/hkust-nlp/deita-quality-scorer)使用方式也和complexity scorer类似，通过prompt形式预测最终的分数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "model_name = \"hkust-nlp/deita-quality-scorer\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_quality(model, tokenizer, input_text, resp_text):\n",
    "    quality_template = (\n",
    "        \"You are a helpful assistant. Please identify the quality score of the Response corresponding to the Question. \\n #Question#:\\n{instruction}\\n#Response#:\\n{output} \\n##Quality: \")\n",
    "    user_input = quality_template.format(\n",
    "        instruction=input_text, output=resp_text)\n",
    "    input_ids = tokenizer.encode(user_input, return_tensors=\"pt\")\n",
    "    print(\"用户输入:\", user_input)\n",
    "    print(\"用户输入分词后:\", input_ids)\n",
    "\n",
    "    max_length = 512\n",
    "    outputs = model.generate(input_ids, max_length=512, num_return_sequences=1,\n",
    "                             return_dict_in_generate=True, output_scores=True)\n",
    "    logprobs_list = outputs.scores[0][0]\n",
    "    print(\"生成结果:\", logprobs_list)\n",
    "    score_logits = []\n",
    "    id2score = {\n",
    "        29896: \"1\",\n",
    "        29906: \"2\",\n",
    "        29941: \"3\",\n",
    "        29946: \"4\",\n",
    "        29945: \"5\",\n",
    "        29953: \"6\"\n",
    "    }\n",
    "    score_template = np.array([1, 2, 3, 4, 5, 6])\n",
    "    for k in id2score:\n",
    "        score_logits.append(logprobs_list[k])\n",
    "    score_logits = np.array(score_logits)\n",
    "    print(\"取出1-6分对应位置模型预测的logits:\", score_logits)\n",
    "    print(\"取出1-6分对应位置模型预测的概率:\", score_npy)\n",
    "    score_npy = score_npy * score_template\n",
    "\n",
    "    score_npy = np.sum(score_npy, axis=0)\n",
    "    print(\"最终分数等于基于概率对1-6分的加权和:\", score_npy)\n",
    "    return score_npy\n",
    "\n",
    "\n",
    "input_text = \"word to describe UI with helpful tooltips\"  # Example Input\n",
    "output_text = \"User-friendly or intuitive UI\"  # Example Output\n",
    "quality_score = infer_quality(model, tokenizer, input_text, output_text)\n",
    "\n",
    "print(quality_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出结果\n",
    "```python\n",
    "用户输入: You are a helpful assistant. Please identify the quality score of the Response corresponding to the Question. \n",
    "#Question#:\n",
    "word to describe UI with helpful tooltips\n",
    "#Response#:\n",
    "User-friendly or intuitive UI \n",
    "##Quality: \n",
    "用户输入分词后: tensor([[    1,   887,   526,   263,  8444, 20255, 29889,  3529, 12439,   278,\n",
    "         11029,  8158,   310,   278, 13291,  6590,   304,   278,   894, 29889,\n",
    "         29871,    13,   396, 16492, 29937, 29901,    13,  1742,   304,  8453,\n",
    "          3740,   411,  8444,  5780,  2034,   567,    13, 29937,  5103, 29937,\n",
    "         29901,    13,  2659, 29899, 18326,   368,   470, 27951,   573,  3740,\n",
    "         29871,    13,  2277, 24399,   537, 29901, 29871]])\n",
    "生成结果: tensor([ -7.2109, -18.8594,  10.8828,  ...,  -3.5664,  -0.7129,   1.8398])\n",
    "取出1-6分对应位置模型预测的logits: [15.90625   23.515625  22.90625   16.40625   12.8203125 10.9375   ]\n",
    "取出1-6分对应位置模型预测的概率: [3.2088807e-04 6.4723665e-01 3.5189646e-01 5.2905490e-04 1.4660470e-05\n",
    " 2.2307599e-06]\n",
    "最终分数等于基于概率对1-6分的加权和: 2.352686479498516\n",
    "2.352686479498516\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要注意的是，为了实际使用时加快推理速度，Deita额外提供了使用[vllm](https://github.com/vllm-project/vllm)进行推理的接口。vllm主要基于PagedAttention、ContinuousBatching等思想提升模型推理的吞吐量，这里的一些相关技术后续会在课程里面详细介绍并讲解其代码，这里只要学会如何使用即可："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 代码文件: deita/src/deita/selection/scorer/base.py\n",
    "if not is_vllm:\n",
    "    self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "    self.model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "else:\n",
    "    from vllm import LLM, SamplingParams\n",
    "    self.llm = LLM(model_name_or_path)\n",
    "    self.sampling_params = SamplingParams(max_tokens=2, logprobs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vllm主要是在大batch推理的时候具有速度优势"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最终我们运行如下的代码为当前数据集打分：\n",
    "```bash\n",
    "# 预测每个数据样本的复杂度\n",
    "SCORETYPE=\"complexity\"\n",
    "DATAPATH=\"./sg_52k.json\"\n",
    "OUTPUTPATH=\"./output/dieta/complexity_sg_52k.json\"\n",
    "MODELPATH=\"hkust-nlp/deita-complexity-scorer\"\n",
    "SCORER=\"llama\"\n",
    "ISVLLM=false\n",
    "\n",
    "python ./score_dataset.py \\\n",
    "    --data_path $DATAPATH \\\n",
    "    --output_path $OUTPUTPATH \\\n",
    "    --score_type $SCORETYPE \\\n",
    "    --scorer $SCORER \\\n",
    "    --scorer_name_or_path $MODELPATH\n",
    "\n",
    "# 预测每个数据样本的质量\n",
    "SCORETYPE=\"quality\"\n",
    "DATAPATH=\"./output/dieta/complexity_sg_52k.json\"\n",
    "OUTPUTPATH=\"./output/dieta/complexity_quality_sg_52k.json\"\n",
    "MODELPATH=\"hkust-nlp/deita-quality-scorer\"\n",
    "SCORER=\"llama\"\n",
    "ISVLLM=false\n",
    "\n",
    "python ./score_dataset.py \\\n",
    "    --data_path $DATAPATH \\\n",
    "    --output_path $OUTPUTPATH \\\n",
    "    --score_type $SCORETYPE \\\n",
    "    --scorer $SCORER \\\n",
    "    --scorer_name_or_path $MODELPATH\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打分器的核心运行脚本可以参考`./run_scorer.sh`。这里的运行速度比较慢，推荐使用vLLM优化或者数据并行计算（参考[llm-eval-harness](https://github.com/EleutherAI/lm-evaluation-harness)）实现，或者直接将数据分片，分别在各张卡上计算完成后再合并。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 数据多样性\n",
    "数据多样性方面通过计算当前样本和样本池内最近邻居的余弦相似度实现，主要核心代码包括：样本嵌入和余弦相似度计算。\n",
    "\n",
    "样本嵌入方面，Deita默认使用Llama1-13B先对整个需要筛选的数据池进行嵌入，将每条数据转换成向量。核心代码如下所示。\n",
    "\n",
    "需要筛选的数据池采用sharegpt格式存储，主要形式如下：\n",
    "\n",
    "```\n",
    "{\n",
    "    \"id\": \"QWJhYvA_0\",\n",
    "    \"conversations\": [\n",
    "      {\n",
    "        \"from\": \"human\",\n",
    "        \"value\": \"Summarize the main ideas of Jeff Walker's Product Launch Formula into bullet points as it pertains to a growth marketing agency implementing these strategies and tactics for their clients...\"\n",
    "      },\n",
    "      {\n",
    "        \"from\": \"gpt\",\n",
    "        \"value\": \"Here are the main ideas of Jeff Walker's Product Launch Formula that can be applied by a growth marketing agency for their clients:\\n\\n1. Identify the target audience and their needs: Understand the \n",
    "ideal customer for the product or service, and create a messaging that resonates with them.\\n2. Pre-launch: Build anticipation and excitement for the launch by creating buzz, gathering testimonials and case studies, a\n",
    "nd using social media to create awareness.\\n3. Launch: Use a well-crafted launch sequence to maximize sales and conversions. This can include offering bonuses, creating scarcity, and using a deadline to create urgency\n",
    ".\\n4. Post-launch: Follow up with customers, gather feedback, and continue to provide value to keep them engaged and loyal.\\n5. Create a product suite: Once the initial product is successful, expand the offering by cr\n",
    "eating additional products or services that address other needs of the same audience.\\n6. Continual optimization: Continually monitor and optimize the launch process and product suite to improve results.\\n7. Build a c\n",
    "ommunity: Use the launch process to build a community of customers who are passionate about the product and can help spread the word.\\n8. Use automation: Use technology and automation to streamline the launch process \n",
    "and improve efficiency.\"\n",
    "      },\n",
    "      {\n",
    "        \"from\": \"human\",\n",
    "        \"value\": \"Summarize the main ideas of Brendon Burchard's Experts Academy into bullet points as it pertains to a growth marketing agency implementing these strategies and tactics for their clients...\"\n",
    "      },\n",
    "      {\n",
    "        \"from\": \"gpt\",\n",
    "        \"value\": \"Here are the main ideas of Brendon Burchard's Experts Academy that can be applied by a growth marketing agency for their clients:\\n\\n1. Identify your expertise: Define your area of expertise and crea\n",
    "te a unique value proposition that differentiates you from others.\\n2. Create a content strategy: Develop a content strategy that showcases your expertise and provides value to your audience. This can include blog pos\n",
    "ts, videos, podcasts, and social media content.\\n3. Build your audience: Use social media, advertising, and other channels to build your audience and increase your visibility.\\n4. Develop your products: Create product\n",
    "s and services that align with your expertise and meet the needs of your audience. These can include digital products, courses, coaching, and consulting services.\\n5. Create a brand: Develop a strong brand that reflec\n",
    "ts your expertise and resonates with your audience.\\n6. Build authority: Establish yourself as an authority in your industry by speaking at events, publishing a book, or appearing on podcasts or TV shows.\\n7. Monetize\n",
    " your expertise: Develop a monetization strategy that leverages your expertise and products to generate revenue.\\n8. Build a team: As your business grows, build a team of experts to help you scale your business and pr\n",
    "ovide excellent service to your clients.\\n9. Continual improvement: Continually improve your products, services, and marketing strategies to stay ahead of the competition and provide the best possible experience for y\n",
    "our clients.\\n10. Focus on impact: Ultimately, focus on making a positive impact in the lives of your clients and using your expertise to make a difference in the world.\"\n",
    "      }\n",
    "}\n",
    "```\n",
    "\n",
    "首先，为嵌入器构建数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文件：deita/src/deita/selection/embedder/clm_embedder.py\n",
    "def encode_samples(self, data):\n",
    "    # 从sharegpt格式的json文件中抽取对话\n",
    "    conversations = [item[\"conversations\"] for item in data]\n",
    "\n",
    "    # 构建databuffer，主要涉及将数据元素修改为huggingface-dataset支持的格式\n",
    "    dataset_buf, data_size = self.create_databuffer(conversations, sort_by_length = True)\n",
    "    # 基于huggingface的Dataset库构建dataset\n",
    "    raw_dataset = Dataset.from_list(dataset_buf)\n",
    "\n",
    "    # 预处理数据，将数据组织成最终训练的格式，包括构建模板（添加标识符，拼接等）、分词\n",
    "    preprocess_func = partial(preprocess, \n",
    "                            conv_template = self.conv_template,\n",
    "                            only_answer = self.only_answer,\n",
    "                            max_length = self.max_length,\n",
    "                            tokenizer = self.tokenizer)\n",
    "    \n",
    "    # 这里的分词过程是先在主进程上进行的，随后广播到各子进程\n",
    "    with self.accelerator.main_process_first():\n",
    "        # dataset的map函数用于对数据集内部的元素进行操作\n",
    "        tokenized_datasets = raw_dataset.map(\n",
    "            preprocess_func,\n",
    "            batched = True,\n",
    "            num_proc = 8,\n",
    "            remove_columns = [\"conversations\", \"specific_length\"],\n",
    "            desc = \"Tokenizing and reformatting instruction data\"\n",
    "        )\n",
    "    # collator函数负责将数据集元素组装成batch，涉及padding、转tensor等操作\n",
    "    data_collator = DataCollatorForSupervisedDataset(tokenizer = self.tokenizer)\n",
    "    # 读取dataloader\n",
    "    dataloader = torch.utils.data.DataLoader(tokenized_datasets, batch_size = self.batch_size_per_device, collate_fn = data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成数据处理后，可以进行batch化的向量嵌入。为了加速推理，这里deita支持多卡数据并行推理，涉及accelerator的一些分布式api以及我们在分布式训练课程中讲解的gather等通信概念，建议仔细阅读代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accelerator准备模型和数据加载器（分布式数据并行推理）\n",
    "model, dataloader = self.accelerator.prepare(self.model, dataloader)\n",
    "\n",
    "all_embeddings_list = []\n",
    "\n",
    "total_samples = len(tokenized_datasets)\n",
    "total_batches = len(dataloader)\n",
    "# 最后一个batch可能放不满，因此要特殊处理\n",
    "last_batch_size = total_samples % self.minibatch_size if total_samples % self.minibatch_size != 0 else self.minibatch_size\n",
    "\n",
    "# 遍历batch进行推理\n",
    "for b_idx, batch in enumerate(tqdm(dataloader, total = len(tokenized_datasets) // self.minibatch_size, disable = not self.accelerator.is_local_main_process)):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    batch_idx = batch[\"idx\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "    # 推理并返回最后一层的隐藏层向量，这里的attention_mask是考虑到了存在pad符，防止干扰\n",
    "    outputs = model(input_ids = batch[\"input_ids\"], attention_mask = batch[\"attention_mask\"], output_hidden_states = True)\n",
    "    \n",
    "    seq_len = attention_mask.sum(1, keepdim = True)\n",
    "    \n",
    "    # 这里需要处理不同分词器对于padding位置的差异\n",
    "    if self.tokenizer.padding_side == \"right\":\n",
    "        # 如果pad符在右侧填充，则需要找到右侧最后一个非pad符的token表示作为当前句子的表示\n",
    "        last_hidden_state = outputs.hidden_states[-1][torch.arange(seq_len.size(0))[:, None], seq_len - 1]\n",
    "    elif self.tokenizer.padding_side == \"left\":    \n",
    "        # 如果pad符在左侧填充，则直接找到末尾token作为当前句子表示即可，一定是非pad符\n",
    "        last_hidden_state = outputs.hidden_states[-1][:, -1]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid padding strategy\")\n",
    "    \n",
    "    sample_idx = batch_idx.tolist()\n",
    "    sample_dict = [{\"embedding\": lst_hs, \"idx\": s_id} for lst_hs, s_id in zip(last_hidden_state.tolist(), sample_idx)]\n",
    "    \n",
    "    # 多GPU数据并行推理，更多分布式推理细节可以参考博客：https://medium.com/@geronimo7/llms-multi-gpu-inference-with-accelerate-5a8333e4c5db\n",
    "    if(self.world_size > 1):\n",
    "        all_process_embeddings = [[] for _ in range(self.world_size)]\n",
    "        # 将多卡的结果按照gpu编号顺序收集，并在主进程上输出到最终的结果对象中\n",
    "        dist.gather_object(sample_dict, all_process_embeddings if dist.get_rank() == 0 else None, dst=0)\n",
    "    else:\n",
    "        all_process_embeddings = [sample_dict]\n",
    "    \n",
    "    # 在主进程上将当前batch的所有结果extend到最终的结果列表\n",
    "    if self.accelerator.is_local_main_process:\n",
    "        if b_idx == total_batches - 1:\n",
    "            for process_list in all_process_embeddings[:last_batch_size]:\n",
    "                all_embeddings_list.extend(process_list)\n",
    "        else:\n",
    "            for process_list in all_process_embeddings:\n",
    "                all_embeddings_list.extend(process_list)   \n",
    "\n",
    "return all_embeddings_list  # 返回最终结果，随后cache到硬盘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在完成数据嵌入后，在实际筛选数据时，我们根据当前数据和已选取数据的余弦相似度最大值进行判断，如果当前数据和最近邻居非常相似，则不能加入候选数据集中（思考：有没有其他策略）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 代码路径：deita/src/deita/selection/embedder/utils.py\n",
    "def filter(self, df):\n",
    "    \n",
    "    logger.info(f\"Data number before filtering: #{len(df)}\")\n",
    "    \n",
    "    df_sorted = self._sort(df)\n",
    "\n",
    "    embeddings = df_sorted[self.embedding_field]  # 当前所有数据的嵌入表示\n",
    "    embeddings = np.array(embeddings.values.tolist())  # 排序后转为numpy数组\n",
    "    \n",
    "    filtered_indices = [0]\n",
    "\n",
    "    start_cnt = 0\n",
    "    for i in tqdm(range(1, embeddings.shape[0], self.batch_size), total = embeddings.shape[0] // self.batch_size):  # 特别注意：这里为了考虑效率问题，原论文支持采用batch化判断相似度\n",
    "\n",
    "        cur_emb = torch.tensor(embeddings[i:i+self.batch_size], dtype = torch.float32).to(self.device)\n",
    "        \n",
    "        if cur_emb.ndim == 4:\n",
    "            cur_emb = cur_emb.squeeze(1).squeeze(1)\n",
    "\n",
    "        if cur_emb.ndim == 1:\n",
    "            cur_emb = cur_emb.unsqueeze(0)\n",
    "\n",
    "        batch_idx = torch.range(i, i + cur_emb.size(0) - 1, dtype = torch.int64).to(self.device)\n",
    "        \n",
    "        existing_emb = embeddings[filtered_indices]\n",
    "\n",
    "        if existing_emb.ndim == 1:\n",
    "            existing_emb = existing_emb.unsqueeze(0)\n",
    "\n",
    "        # 计算当前batch内数据和已经被筛除过的数据的余弦距离，和任一数据过近则筛除\n",
    "        # 需要注意这里distance_chunk_by_chunk函数是分桶计算的，防止向量维度过高导致运算速度过慢\n",
    "        distance_existed = self.distance_chunk_by_chunk(existing_emb, cur_emb)\n",
    "        distance_existed_bool = torch.any(distance_existed > self.threshold, dim = 1)\n",
    "        \n",
    "        # 计算当前batch内数据各自之间的余弦距离，和任一数据过近则筛除\n",
    "        distance_cur = self.distance_chunk_by_chunk(cur_emb, cur_emb)\n",
    "\n",
    "        # 只取上三角or下三角并且不考虑对角线（不考虑自身之间，并且避免重复计算）\n",
    "        distance_cur = distance_cur.tril(-1)\n",
    "        \n",
    "        distance_cur_bool = torch.any(distance_cur > self.threshold, dim = 1)\n",
    "        \n",
    "        # 二者取并集，代表需要被筛除的元素\n",
    "        distance_bool = distance_existed_bool | distance_cur_bool\n",
    "        \n",
    "        # 留下的数据加入filtered_indices\n",
    "        filtered_indices.extend(batch_idx[~distance_bool].tolist())\n",
    "\n",
    "        if len(filtered_indices) - start_cnt > 1000:\n",
    "            logger.info(\"Now data number: #{}\".format(len(filtered_indices)))\n",
    "            start_cnt = len(filtered_indices)\n",
    "\n",
    "        if self.data_size > -1:\n",
    "            if len(filtered_indices) >= self.data_size:\n",
    "                break\n",
    "        \n",
    "    # 取出被保留的元素\n",
    "    df_filtered = df_sorted.iloc[filtered_indices]        \n",
    "    logger.info(f\"Data number after filtering: #{len(df_filtered)}\")\n",
    "    \n",
    "    if self.data_size > -1:\n",
    "        return df_filtered[:self.data_size]  # 完成过滤后的数据，只取指定的data_size规模的数据加入最终的数据集，因为这里按照complexity_score*quality_score排序过了\n",
    "    else:\n",
    "        return df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算向量余弦相似度的代码也可以学习下，先将向量模长归一化为1，然后求向量乘积即可（无需再除以向量模长之积了，因为已经归一化了）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 代码路径：deita/src/deita/selection/embedder/utils.py\n",
    "def compute_distance(self, matrix, matrix_2):\n",
    "    \"\"\"\n",
    "        使用pytorch计算余弦距离\n",
    "    \"\"\"\n",
    "    \n",
    "    # 如果需要对向量进行归一化\n",
    "    if self.normalize_emb:\n",
    "        # 对每个向量进行归一化处理，使其长度为1\n",
    "        matrix = matrix / matrix.norm(dim=1)[:, None]\n",
    "        matrix_2 = matrix_2 / matrix_2.norm(dim=1)[:, None]\n",
    "\n",
    "    # 如果距离度量方法为余弦相似度\n",
    "    if self.distance_metric == 'cosine':\n",
    "        # 对每个向量进行归一化处理，使其长度为1\n",
    "        matrix_norm = matrix / matrix.norm(dim=1)[:, None]\n",
    "        matrix_2_norm = matrix_2 / matrix_2.norm(dim=1)[:, None]\n",
    "        # 计算两个矩阵的余弦相似度\n",
    "        return torch.mm(matrix_norm, matrix_2_norm.t())\n",
    "    # 如果距离度量方法为曼哈顿距离\n",
    "    elif self.distance_metric == 'manhattan':\n",
    "        # 计算两个矩阵的曼哈顿距离\n",
    "        return torch.cdist(matrix[None], matrix_2[None], p = 1).squeeze(0)\n",
    "    else:\n",
    "        # 如果指定的距离度量方法不支持，抛出错误\n",
    "        raise ValueError(\"Metric not supported. Only support cosine and manhattan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完整的嵌入代码可以参见：`./embed_datasets.py`，启动命令可见：`run_embed_datasets.sh`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 整体使用\n",
    "\n",
    "在完成嵌入后，可以使用`run_combined_filter.sh`这个命令文件，基于已经计算好的数据分数和嵌入向量文件，进行筛选。\n",
    "\n",
    "完整的数据打分-嵌入-筛选流程可参考`./deita_tutorial/run_deita.sh`文件运行。\n",
    "\n",
    "### 备注\n",
    "Deita工具的使用对于显卡要求较高，从这个[链接](https://huggingface.co/datasets/hkust-nlp/deita-10k-v0)下载筛选后的数据进行模型微调的实验\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
